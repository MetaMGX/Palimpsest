{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab417299",
   "metadata": {},
   "source": [
    "\n",
    "# Research on String Matching and Semantic Matching Techniques for Palimpsest\n",
    "\n",
    "This notebook summarizes research on efficient algorithms for string matching and semantic matching techniques\n",
    "applicable to the Palimpsest text analysis tool.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e0cd2",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "Palimpsest is a text analysis tool designed for comparing and analyzing large documents. This research explores\n",
    "efficient algorithms for two key components:\n",
    "\n",
    "1. **String Matching**: Techniques for identifying exact or approximate text matches across documents\n",
    "2. **Semantic Matching**: Methods for finding content with similar meaning regardless of exact wording\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69119fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Function to display markdown content\n",
    "def md(text):\n",
    "    display(Markdown(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412397c",
   "metadata": {},
   "source": [
    "\n",
    "## 1. String Matching Techniques\n",
    "\n",
    "String matching algorithms find occurrences of exact or approximate patterns within text documents.\n",
    "The following techniques have been analyzed for potential use in Palimpsest:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcaf77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# String matching algorithms comparison\n",
    "algorithms = {\n",
    "    \"Exact Matching\": [\n",
    "        (\"KMP (Knuth-Morris-Pratt)\", \"Linear time pattern matching\", \"O(m+n)\", \"Low\", \"Small-Medium\"),\n",
    "        (\"Boyer-Moore\", \"Skips characters for efficiency\", \"O(m*n) worst, O(n/m) best\", \"Low\", \"Small-Medium\"),\n",
    "        (\"Rabin-Karp\", \"Rolling hash function\", \"O(m*n) worst, O(m+n) average\", \"Medium\", \"Small-Medium\"),\n",
    "        (\"Aho-Corasick\", \"Multiple pattern matching\", \"O(n + m + z)\", \"High\", \"Small-Medium\")\n",
    "    ],\n",
    "    \n",
    "    \"Suffix Data Structures\": [\n",
    "        (\"Suffix Trees\", \"Tree containing all suffixes\", \"O(n) construction, O(m) search\", \"High\", \"Small\"),\n",
    "        (\"Suffix Arrays\", \"Sorted array of suffixes\", \"O(n log n) construction, O(m log n) search\", \"Medium\", \"Medium-Large\"),\n",
    "        (\"Enhanced Suffix Arrays\", \"Suffix arrays with additional tables\", \"O(n) query after O(n) preprocessing\", \"Medium\", \"Medium-Large\"),\n",
    "        (\"FM-Index\", \"Compressed full-text index\", \"Fast search with compressed storage\", \"High\", \"Large\")\n",
    "    ],\n",
    "    \n",
    "    \"Approximate Matching\": [\n",
    "        (\"Edit Distance (Levenshtein)\", \"Min operations to transform strings\", \"O(m*n)\", \"Low\", \"Small\"),\n",
    "        (\"Bitap Algorithm\", \"Uses bit parallelism\", \"O(m*n/w) where w is word size\", \"Medium\", \"Small-Medium\"),\n",
    "        (\"N-gram Overlap\", \"Compare shared n-grams\", \"O(m+n)\", \"Low\", \"Medium-Large\"),\n",
    "        (\"Locality Sensitive Hashing\", \"Hash similar items to same buckets\", \"Dependent on implementation\", \"High\", \"Large\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display the algorithms with their properties\n",
    "for category, techniques in algorithms.items():\n",
    "    md(f\"### {category}\")\n",
    "    \n",
    "    table_data = []\n",
    "    for name, desc, complexity, implementation, doc_size in techniques:\n",
    "        table_data.append([name, desc, complexity, implementation, doc_size])\n",
    "    \n",
    "    df = pd.DataFrame(table_data, \n",
    "                     columns=[\"Algorithm\", \"Description\", \"Time Complexity\", \n",
    "                              \"Implementation Complexity\", \"Suitable Document Size\"])\n",
    "    display(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example implementation of string matching using suffix arrays\n",
    "def create_suffix_array(text):\n",
    "    \"\"\"\n",
    "    Create a suffix array from text\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        \n",
    "    Returns:\n",
    "        Sorted array of suffix indices\n",
    "    \"\"\"\n",
    "    # Create list of all suffixes with their starting positions\n",
    "    suffixes = [(i, text[i:]) for i in range(len(text))]\n",
    "    \n",
    "    # Sort by suffix\n",
    "    suffixes.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Extract and return just the indices\n",
    "    return [pos for pos, _ in suffixes]\n",
    "\n",
    "def search_suffix_array(text, suffix_array, pattern):\n",
    "    \"\"\"\n",
    "    Search for pattern in text using suffix array\n",
    "    \n",
    "    Args:\n",
    "        text: Text to search in\n",
    "        suffix_array: Suffix array of text\n",
    "        pattern: Pattern to search for\n",
    "        \n",
    "    Returns:\n",
    "        List of positions where pattern occurs\n",
    "    \"\"\"\n",
    "    # Binary search for the pattern\n",
    "    pattern_len = len(pattern)\n",
    "    text_len = len(text)\n",
    "    left, right = 0, len(suffix_array) - 1\n",
    "    results = []\n",
    "    \n",
    "    # Find the first occurrence\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        suffix_pos = suffix_array[mid]\n",
    "        suffix = text[suffix_pos:suffix_pos+pattern_len]\n",
    "        \n",
    "        if pattern == suffix:\n",
    "            # Found a match, now find all matches\n",
    "            results.append(suffix_pos)\n",
    "            # Check to the left\n",
    "            i = mid - 1\n",
    "            while i >= 0:\n",
    "                suffix_pos = suffix_array[i]\n",
    "                suffix = text[suffix_pos:suffix_pos+pattern_len]\n",
    "                if pattern == suffix:\n",
    "                    results.append(suffix_pos)\n",
    "                else:\n",
    "                    break\n",
    "                i -= 1\n",
    "            # Check to the right\n",
    "            i = mid + 1\n",
    "            while i < len(suffix_array):\n",
    "                suffix_pos = suffix_array[i]\n",
    "                suffix = text[suffix_pos:suffix_pos+pattern_len]\n",
    "                if pattern == suffix:\n",
    "                    results.append(suffix_pos)\n",
    "                else:\n",
    "                    break\n",
    "                i += 1\n",
    "            break\n",
    "        elif pattern < suffix:\n",
    "            right = mid - 1\n",
    "        else:\n",
    "            left = mid + 1\n",
    "    \n",
    "    return sorted(results)\n",
    "\n",
    "# Example usage\n",
    "example_text = \"banana\"\n",
    "suffix_array = create_suffix_array(example_text)\n",
    "print(f\"Text: {example_text}\")\n",
    "print(f\"Suffix array: {suffix_array}\")\n",
    "print(f\"Suffixes in sorted order:\")\n",
    "for i in suffix_array:\n",
    "    print(f\"  {i}: {example_text[i:]}\")\n",
    "    \n",
    "pattern = \"na\"\n",
    "matches = search_suffix_array(example_text, suffix_array, pattern)\n",
    "print(f\"Matches for '{pattern}': {matches}\")\n",
    "\n",
    "# For larger examples, we might use a library implementation\n",
    "md(\"### Libraries for Efficient String Matching\")\n",
    "md(\"\"\"\n",
    "For production use in Palimpsest, consider these libraries:\n",
    "- **Suffix Arrays**: `pysuffix` or `numpy` based implementations\n",
    "- **FM-Index**: `pyFM` for Python implementation\n",
    "- **Approximate Matching**: `fuzzywuzzy`, `rapidfuzz` or `difflib`\n",
    "- **Multi-pattern**: `pyahocorasick` for Aho-Corasick implementation\n",
    "\"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7bc157",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Semantic Matching Techniques\n",
    "\n",
    "Semantic matching identifies textual content with similar meaning regardless of the exact words used.\n",
    "This is essential for detecting conceptual similarities, paraphrasing, and translations.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Semantic matching techniques comparison\n",
    "semantic_techniques = {\n",
    "    \"Classical Approaches\": [\n",
    "        (\"TF-IDF + Cosine Similarity\", \"Compare document vectors\", \"Fast\", \"Low\", \"Limited semantic understanding\"),\n",
    "        (\"LSA (Latent Semantic Analysis)\", \"SVD on term-document matrix\", \"Medium\", \"Medium\", \"Captures some semantic relationships\"),\n",
    "        (\"LDA (Latent Dirichlet Allocation)\", \"Topic modeling\", \"Slow\", \"Medium\", \"Good for topic similarity\")\n",
    "    ],\n",
    "    \n",
    "    \"Word Embeddings\": [\n",
    "        (\"Word2Vec\", \"Neural word embeddings\", \"Fast once trained\", \"Medium\", \"Context-free word meaning\"),\n",
    "        (\"GloVe\", \"Global word vectors\", \"Fast once trained\", \"Medium\", \"Global co-occurrence statistics\"),\n",
    "        (\"FastText\", \"Subword embeddings\", \"Fast\", \"Medium\", \"Handles OOV words & morphology\")\n",
    "    ],\n",
    "    \n",
    "    \"Contextual Embeddings\": [\n",
    "        (\"BERT\", \"Bidirectional Transformer\", \"Slow\", \"High\", \"Context-dependent meaning\"),\n",
    "        (\"RoBERTa\", \"Optimized BERT\", \"Slow\", \"High\", \"Improved training methodology\"),\n",
    "        (\"DistilBERT\", \"Distilled BERT\", \"Medium\", \"Medium\", \"Faster with slight accuracy drop\"),\n",
    "        (\"Sentence-BERT\", \"Sentence embeddings\", \"Medium\", \"Medium\", \"Optimized for sentence similarity\")\n",
    "    ],\n",
    "    \n",
    "    \"Advanced Techniques\": [\n",
    "        (\"Cross-Encoders\", \"Direct pair classification\", \"Very Slow\", \"High\", \"Highest accuracy for pairs\"),\n",
    "        (\"Siamese Networks\", \"Neural text similarity\", \"Medium\", \"High\", \"Trained for similarity tasks\"),\n",
    "        (\"Universal Sentence Encoder\", \"Sentence embeddings\", \"Medium\", \"Medium\", \"Multi-task trained embeddings\"),\n",
    "        (\"MPNet\", \"Masked & permuted pre-training\", \"Slow\", \"High\", \"State-of-art embeddings\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display the semantic techniques with their properties\n",
    "for category, techniques in semantic_techniques.items():\n",
    "    md(f\"### {category}\")\n",
    "    \n",
    "    table_data = []\n",
    "    for name, desc, speed, complexity, pros_cons in techniques:\n",
    "        table_data.append([name, desc, speed, complexity, pros_cons])\n",
    "    \n",
    "    df = pd.DataFrame(table_data, \n",
    "                     columns=[\"Technique\", \"Description\", \"Processing Speed\", \n",
    "                              \"Implementation Complexity\", \"Quality\"])\n",
    "    display(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example implementation of TF-IDF based semantic matching\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def tfidf_similarity(documents):\n",
    "    \"\"\"\n",
    "    Calculate pairwise TF-IDF cosine similarity between documents\n",
    "    \n",
    "    Args:\n",
    "        documents: List of text documents\n",
    "        \n",
    "    Returns:\n",
    "        Similarity matrix\n",
    "    \"\"\"\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    # Fit and transform the documents\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "# Example \n",
    "example_docs = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A fast auburn fox leaps above the sleepy canine\",\n",
    "    \"Python is a popular programming language\",\n",
    "    \"Programming in Python is widely adopted in data science\"\n",
    "]\n",
    "\n",
    "sim_matrix = tfidf_similarity(example_docs)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "doc_names = [f\"Doc {i+1}\" for i in range(len(example_docs))]\n",
    "sim_df = pd.DataFrame(sim_matrix, index=doc_names, columns=doc_names)\n",
    "display(sim_df)\n",
    "\n",
    "# Visualize the similarity matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(sim_df, annot=True, cmap='Blues')\n",
    "plt.title('Document Similarity Matrix (TF-IDF)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "md(\"### Modern Semantic Matching Example\")\n",
    "md(\"\"\"\n",
    "For production implementation in Palimpsest, sentence transformers provide an excellent balance of accuracy and speed:\n",
    "- **sentence-transformers**: Pre-trained models specifically for text similarity\n",
    "- **HuggingFace Transformers**: Access to state-of-the-art language models\n",
    "- **FAISS**: Fast similarity search for large document collections\n",
    "- **Annoy**: Approximate nearest neighbors for efficient retrieval\n",
    "\"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1117a759",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Recommendations for Implementation\n",
    "\n",
    "Based on the research conducted, we recommend the following implementation approach for Palimpsest:\n",
    "\n",
    "1. **Multi-tier Matching Strategy**:\n",
    "   - Start with fast approximate methods (LSH, n-grams) to identify candidate matches\n",
    "   - Apply more precise algorithms to promising candidates\n",
    "   - Use semantic matching for concept-level similarity detection\n",
    "\n",
    "2. **Scalability Considerations**:\n",
    "   - Use data structures that support incremental updates (e.g., suffix arrays)\n",
    "   - Implement document partitioning for parallel processing\n",
    "   - Consider compression techniques for large document collections\n",
    "\n",
    "3. **Hybrid Approach**:\n",
    "   - Combine string and semantic matching for comprehensive document comparison\n",
    "   - Weight different similarity metrics based on use case requirements\n",
    "   - Allow users to configure precision vs. recall trade-offs\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
