{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807955fd",
   "metadata": {},
   "source": [
    "\n",
    "# Research on Syntactic, Structural, and Plot Analysis Techniques for Palimpsest\n",
    "\n",
    "This notebook summarizes research on advanced techniques for analyzing text documents at syntactic, \n",
    "structural, and narrative levels for the Palimpsest text analysis tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f89612d",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "Palimpsest is a text analysis tool designed for comparing and analyzing large documents. \n",
    "This research explores techniques for analyzing documents beyond simple string and semantic matching:\n",
    "\n",
    "1. **Syntactic Analysis**: Techniques for examining grammatical structure and syntactic patterns\n",
    "2. **Structural Analysis**: Methods for analyzing document organization and structure \n",
    "3. **Plot and Narrative Analysis**: Approaches for understanding narrative elements and plot development\n",
    "\n",
    "These higher-level analyses complement the string matching and semantic matching techniques\n",
    "covered in our previous research notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd0672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from IPython.display import Markdown, display\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to display markdown content\n",
    "def md(text):\n",
    "    display(Markdown(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816737e0",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Syntactic Analysis Techniques\n",
    "\n",
    "Syntactic analysis examines the grammatical structure of texts, identifying patterns in \n",
    "sentence construction, grammar usage, and syntactic style. These techniques help identify \n",
    "authorial fingerprints and stylistic influences across documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Syntactic analysis techniques comparison\n",
    "syntactic_techniques = {\n",
    "    \"Part-of-Speech Analysis\": [\n",
    "        (\"POS Tagging Distribution\", \"Statistical analysis of POS tag frequencies\", \"spaCy, NLTK\", \"Medium\"),\n",
    "        (\"Syntactic n-grams\", \"Sequences of POS tags rather than words\", \"NLTK, custom\", \"Medium\"),\n",
    "        (\"Dependency Parse Tree Analysis\", \"Examining grammatical relations between words\", \"spaCy, Stanford NLP\", \"High\"),\n",
    "        (\"Constituency Parsing\", \"Hierarchical phrase structure analysis\", \"NLTK, AllenNLP\", \"High\")\n",
    "    ],\n",
    "    \n",
    "    \"Grammar and Style Analysis\": [\n",
    "        (\"Readability Metrics\", \"Flesch-Kincaid, Coleman-Liau, SMOG indexes\", \"textstat\", \"Low\"),\n",
    "        (\"Sentence Complexity Analysis\", \"Clausal density, subordination\", \"spaCy + custom rules\", \"Medium\"),\n",
    "        (\"Grammar Rule Usage\", \"Passive voice, nominalization frequencies\", \"Language Tool, custom rules\", \"Medium\"),\n",
    "        (\"Rhetorical Device Detection\", \"Anaphora, epistrophe, etc.\", \"Custom implementations\", \"High\")\n",
    "    ],\n",
    "    \n",
    "    \"Syntactic Similarity Measurement\": [\n",
    "        (\"Tree Edit Distance\", \"Comparing parse trees\", \"APTED, Zhang-Shasha\", \"High\"),\n",
    "        (\"Syntactic Embeddings\", \"Neural representations of syntactic structures\", \"Berkeley Neural Parser\", \"High\"),\n",
    "        (\"Function Word Stylometry\", \"Analysis of function word patterns\", \"Custom implementations\", \"Medium\"),\n",
    "        (\"Syntactic Motif Analysis\", \"Recurring grammatical patterns\", \"Custom graph-based analysis\", \"High\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display the syntactic techniques with their properties\n",
    "for category, techniques in syntactic_techniques.items():\n",
    "    md(f\"### {category}\")\n",
    "    \n",
    "    table_data = []\n",
    "    for name, desc, tools, complexity in techniques:\n",
    "        table_data.append([name, desc, tools, complexity])\n",
    "    \n",
    "    df = pd.DataFrame(table_data, \n",
    "                     columns=[\"Technique\", \"Description\", \"Tools\", \"Implementation Complexity\"])\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ace88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example implementation of syntactic fingerprinting\n",
    "def create_syntactic_fingerprint(text):\n",
    "    \"\"\"\n",
    "    Creates a syntactic fingerprint from text based on POS patterns\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of syntactic features\n",
    "    \"\"\"\n",
    "    # We'll use NLTK for POS tagging since spaCy might not be available\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    \n",
    "    try:\n",
    "        # Download necessary NLTK resources if not already available\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    except:\n",
    "        print(\"NLTK resources could not be downloaded, continuing with limited functionality\")\n",
    "    \n",
    "    # Process the text\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokens = [word_tokenize(sent) for sent in sentences]\n",
    "    pos_tags = [nltk.pos_tag(sent) for sent in tokens]\n",
    "    \n",
    "    # Initialize fingerprint features\n",
    "    fingerprint = {\n",
    "        'pos_trigram_dist': defaultdict(int),\n",
    "        'sentence_length_dist': defaultdict(int),\n",
    "        'function_word_ratio': 0,\n",
    "        'punctuation_ratio': 0\n",
    "    }\n",
    "    \n",
    "    # Calculate POS trigrams\n",
    "    all_pos = [tag for sent in pos_tags for _, tag in sent]\n",
    "    for i in range(len(all_pos) - 2):\n",
    "        trigram = (all_pos[i], all_pos[i+1], all_pos[i+2])\n",
    "        fingerprint['pos_trigram_dist'][trigram] += 1\n",
    "    \n",
    "    # Calculate sentence length distribution\n",
    "    for sent in tokens:\n",
    "        length = len(sent)\n",
    "        bin_size = 10\n",
    "        bin_index = length // bin_size\n",
    "        fingerprint['sentence_length_dist'][bin_index] += 1\n",
    "    \n",
    "    # Calculate function word ratio\n",
    "    function_pos = {'IN', 'DT', 'CC', 'PRP', 'PRP$', 'WDT', 'WP', 'WRB', 'TO'}\n",
    "    function_words = sum(1 for sent in pos_tags for word, tag in sent if tag in function_pos)\n",
    "    total_words = sum(len(sent) for sent in tokens)\n",
    "    fingerprint['function_word_ratio'] = function_words / total_words if total_words > 0 else 0\n",
    "    \n",
    "    # Count punctuation\n",
    "    punctuation = sum(1 for sent in tokens for word in sent if word in '.,:;!?()[]{}\"\"\\'')\n",
    "    fingerprint['punctuation_ratio'] = punctuation / total_words if total_words > 0 else 0\n",
    "    \n",
    "    return fingerprint\n",
    "\n",
    "# Example text\n",
    "example_text1 = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. This sentence contains every letter of the alphabet.\n",
    "The concept of a pangram has been used by font designers and others to display examples of fonts.\n",
    "\"\"\"\n",
    "\n",
    "example_text2 = \"\"\"\n",
    "A swift amber-colored vulpine animal leaps across the indolent canine. This particular sequence \n",
    "of words incorporates all 26 letters found in English alphabet.\n",
    "\"\"\"\n",
    "\n",
    "# Create fingerprints\n",
    "fingerprint1 = create_syntactic_fingerprint(example_text1)\n",
    "fingerprint2 = create_syntactic_fingerprint(example_text2)\n",
    "\n",
    "# Display some key features\n",
    "print(\"Sample of POS trigrams in Text 1:\")\n",
    "top_trigrams1 = sorted(fingerprint1['pos_trigram_dist'].items(), \n",
    "                       key=lambda x: x[1], reverse=True)[:5]\n",
    "for trigram, count in top_trigrams1:\n",
    "    print(f\"  {trigram}: {count}\")\n",
    "\n",
    "print(\"\\nSample of POS trigrams in Text 2:\")\n",
    "top_trigrams2 = sorted(fingerprint2['pos_trigram_dist'].items(), \n",
    "                       key=lambda x: x[1], reverse=True)[:5]\n",
    "for trigram, count in top_trigrams2:\n",
    "    print(f\"  {trigram}: {count}\")\n",
    "\n",
    "print(\"\\nFunction word ratio:\")\n",
    "print(f\"  Text 1: {fingerprint1['function_word_ratio']:.3f}\")\n",
    "print(f\"  Text 2: {fingerprint2['function_word_ratio']:.3f}\")\n",
    "\n",
    "md(\"### Syntactic Similarity Libraries and Tools\")\n",
    "md(\"\"\"\n",
    "Key libraries for syntactic analysis in Palimpsest:\n",
    "- **spaCy**: Industrial-strength NLP with dependency parsing and POS tagging\n",
    "- **NLTK**: Natural Language Toolkit with parsing and POS analysis\n",
    "- **textstat**: Library for calculating readability metrics\n",
    "- **StanfordNLP/CoreNLP**: Advanced syntax analysis tools\n",
    "- **Pattern**: Toolkit for stylometry and grammatical analysis\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff4d0d5",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Structural Analysis Techniques\n",
    "\n",
    "Structural analysis examines how text is organized at higher levels, including paragraph structures,\n",
    "section organization, and document architecture. These techniques help identify structural similarities\n",
    "and differences between texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81581c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Structural analysis techniques comparison\n",
    "structural_techniques = {\n",
    "    \"Document Segmentation\": [\n",
    "        (\"TextTiling\", \"Topic-based text segmentation\", \"NLTK\", \"Medium\"),\n",
    "        (\"C99 Algorithm\", \"Matrix-based text segmentation\", \"Custom\", \"Medium\"),\n",
    "        (\"TopicTiling\", \"Topic model-based segmentation\", \"Custom + LDA\", \"High\"),\n",
    "        (\"BERTopic Segmentation\", \"BERT-based topic segmentation\", \"BERTopic\", \"High\")\n",
    "    ],\n",
    "    \n",
    "    \"Hierarchical Structure Analysis\": [\n",
    "        (\"Section Hierarchy Detection\", \"Identifying document hierarchy from headings\", \"Custom + NLP\", \"Medium\"),\n",
    "        (\"Argumentation Mining\", \"Identifying argument structures\", \"ArguMiner, ATHAR\", \"High\"),\n",
    "        (\"Rhetorical Structure Theory\", \"Discourse relations between text segments\", \"RST parsers\", \"Very High\"),\n",
    "        (\"Document Architecture Analysis\", \"Identifying structural components\", \"GATE, Custom\", \"High\")\n",
    "    ],\n",
    "    \n",
    "    \"Structural Comparison\": [\n",
    "        (\"Structural Fingerprinting\", \"Generate document structure fingerprints\", \"Custom implementations\", \"Medium\"),\n",
    "        (\"XML/DOM Distance\", \"Tree-based structural comparison\", \"Tree edit distance algorithms\", \"Medium\"),\n",
    "        (\"Structure Visualization\", \"Visualizing document structure\", \"NetworkX, D3.js\", \"Medium\"),\n",
    "        (\"Fractal Structure Analysis\", \"Self-similarity in text structure\", \"Custom implementations\", \"High\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display the structural techniques with their properties\n",
    "for category, techniques in structural_techniques.items():\n",
    "    md(f\"### {category}\")\n",
    "    \n",
    "    table_data = []\n",
    "    for name, desc, tools, complexity in techniques:\n",
    "        table_data.append([name, desc, tools, complexity])\n",
    "    \n",
    "    df = pd.DataFrame(table_data, \n",
    "                     columns=[\"Technique\", \"Description\", \"Tools\", \"Implementation Complexity\"])\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example implementation of document structure extraction\n",
    "import re\n",
    "\n",
    "def extract_document_structure(text):\n",
    "    \"\"\"\n",
    "    Extract structural components from a document\n",
    "    \n",
    "    Args:\n",
    "        text: Document text\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary representing document structure\n",
    "    \"\"\"\n",
    "    # Split into paragraphs\n",
    "    paragraphs = [p for p in text.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    structure = {\n",
    "        'paragraph_count': len(paragraphs),\n",
    "        'paragraph_lengths': [len(p) for p in paragraphs],\n",
    "        'section_hierarchy': [],\n",
    "        'section_depths': [],\n",
    "        'section_relations': []\n",
    "    }\n",
    "    \n",
    "    # Simple section detection (can be enhanced with ML)\n",
    "    section_pattern = re.compile(r'^(#{1,6})\\s+(.+)$', re.MULTILINE)\n",
    "    sections = [(len(m.group(1)), m.group(2)) for m in section_pattern.finditer(text)]\n",
    "    \n",
    "    if sections:\n",
    "        structure['section_hierarchy'] = sections\n",
    "        structure['section_depths'] = [depth for depth, _ in sections]\n",
    "        \n",
    "        # Create section hierarchy graph\n",
    "        G = nx.DiGraph()\n",
    "        stack = [(0, \"ROOT\", 0)]  # (index, title, depth)\n",
    "        \n",
    "        for i, (depth, title) in enumerate(sections):\n",
    "            # Pop elements from stack that are at same or greater depth\n",
    "            while stack and stack[-1][2] >= depth:\n",
    "                stack.pop()\n",
    "                \n",
    "            if stack:\n",
    "                parent_idx = stack[-1][0]\n",
    "                G.add_edge(parent_idx, i + 1)  # +1 because 0 is ROOT\n",
    "                \n",
    "            stack.append((i + 1, title, depth))\n",
    "            \n",
    "        structure['section_relations'] = list(G.edges())\n",
    "    \n",
    "    return structure\n",
    "\n",
    "# Example document with structure\n",
    "example_doc = \"\"\"\n",
    "# Document Title\n",
    "\n",
    "This is an introduction paragraph that sets up the document.\n",
    "It spans multiple lines to form a single paragraph.\n",
    "\n",
    "## First Section\n",
    "\n",
    "This is the content of the first section.\n",
    "It might contain some important information.\n",
    "\n",
    "### Subsection 1.1\n",
    "\n",
    "Deeper nested content with more specific details.\n",
    "\n",
    "## Second Section\n",
    "\n",
    "Another top-level section with its own content.\n",
    "This demonstrates a different branch in the document hierarchy.\n",
    "\n",
    "### Subsection 2.1\n",
    "\n",
    "More detailed information in a subsection.\n",
    "\n",
    "#### Even Deeper Subsection\n",
    "\n",
    "We can go quite deep in the hierarchy if needed.\n",
    "\"\"\"\n",
    "\n",
    "# Extract structure\n",
    "doc_structure = extract_document_structure(example_doc)\n",
    "\n",
    "# Display structure information\n",
    "print(f\"Document Structure Analysis:\")\n",
    "print(f\"Paragraph count: {doc_structure['paragraph_count']}\")\n",
    "print(f\"Average paragraph length: {sum(doc_structure['paragraph_lengths']) / len(doc_structure['paragraph_lengths']):.1f} characters\")\n",
    "print(f\"\\nSection hierarchy:\")\n",
    "for depth, title in doc_structure['section_hierarchy']:\n",
    "    print(f\"{'  ' * (depth-1)}{title}\")\n",
    "\n",
    "# Visualize the section hierarchy\n",
    "if doc_structure['section_relations']:\n",
    "    G = nx.DiGraph(doc_structure['section_relations'])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    pos = nx.spring_layout(G)\n",
    "    \n",
    "    # Create labels\n",
    "    labels = {0: \"ROOT\"}\n",
    "    for i, (depth, title) in enumerate(doc_structure['section_hierarchy']):\n",
    "        labels[i + 1] = title\n",
    "    \n",
    "    nx.draw(G, pos, with_labels=False, node_size=500, node_color=\"lightblue\")\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=10)\n",
    "    plt.title(\"Document Section Hierarchy\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c2858",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Plot and Narrative Analysis\n",
    "\n",
    "Plot and narrative analysis focuses on understanding how stories unfold, identifying \n",
    "character relationships, plot arcs, and narrative techniques. These approaches help identify \n",
    "similar narrative structures across texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1491af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Narrative analysis techniques comparison\n",
    "narrative_techniques = {\n",
    "    \"Character Analysis\": [\n",
    "        (\"Named Entity Recognition\", \"Identifying characters and places\", \"spaCy, NLTK\", \"Low\"),\n",
    "        (\"Character Network Analysis\", \"Social network of characters\", \"NetworkX + NER\", \"Medium\"),\n",
    "        (\"Character Attribute Extraction\", \"Identifying character traits\", \"Custom + NLP\", \"High\"),\n",
    "        (\"Dialogue Attribution\", \"Associating dialogue with speakers\", \"Custom ML + rules\", \"High\")\n",
    "    ],\n",
    "    \n",
    "    \"Plot Structure Analysis\": [\n",
    "        (\"Event Detection\", \"Identifying key events in narrative\", \"Custom NLP + ML\", \"High\"),\n",
    "        (\"Story Arc Identification\", \"Detecting narrative arcs\", \"Custom + sentiment analysis\", \"High\"),\n",
    "        (\"Narrative Tempo Analysis\", \"Pacing of narrative\", \"Custom implementation\", \"Medium\"),\n",
    "        (\"Plot Comparison\", \"Compare plot structures\", \"Custom algorithms\", \"Very High\")\n",
    "    ],\n",
    "    \n",
    "    \"Narrative Techniques\": [\n",
    "        (\"Focalization Analysis\", \"Point of view detection\", \"Custom ML classifiers\", \"High\"),\n",
    "        (\"Temporal Structure\", \"Chronology, flashbacks, etc.\", \"Custom implementation\", \"High\"),\n",
    "        (\"Narrative Modes\", \"Showing vs. telling, etc.\", \"Custom rules + ML\", \"High\"),\n",
    "        (\"Intertextuality Detection\", \"References to other texts\", \"Knowledge bases + NLP\", \"Very High\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display the narrative techniques with their properties\n",
    "for category, techniques in narrative_techniques.items():\n",
    "    md(f\"### {category}\")\n",
    "    \n",
    "    table_data = []\n",
    "    for name, desc, tools, complexity in techniques:\n",
    "        table_data.append([name, desc, tools, complexity])\n",
    "    \n",
    "    df = pd.DataFrame(table_data, \n",
    "                     columns=[\"Technique\", \"Description\", \"Tools\", \"Implementation Complexity\"])\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7652f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example implementation of narrative element extraction\n",
    "import re\n",
    "\n",
    "def extract_narrative_elements(text):\n",
    "    \"\"\"\n",
    "    Extract narrative elements from text\n",
    "    \n",
    "    Args:\n",
    "        text: Document text\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of narrative features\n",
    "    \"\"\"\n",
    "    # Use NLTK for NER if spaCy is not available\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    \n",
    "    try:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "        nltk.download('maxent_ne_chunker', quiet=True)\n",
    "        nltk.download('words', quiet=True)\n",
    "    except:\n",
    "        print(\"NLTK resources could not be downloaded, continuing with limited functionality\")\n",
    "    \n",
    "    # Process the text\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokens = [word_tokenize(sent) for sent in sentences]\n",
    "    pos_tags = [nltk.pos_tag(sent) for sent in tokens]\n",
    "    \n",
    "    # Try to use NLTK's NER capabilities\n",
    "    try:\n",
    "        ner_chunks = [nltk.ne_chunk(sent) for sent in pos_tags]\n",
    "        # Extract named entities\n",
    "        entities = []\n",
    "        for tree in ner_chunks:\n",
    "            for subtree in tree:\n",
    "                if hasattr(subtree, 'label'):\n",
    "                    entity = ' '.join([word for word, tag in subtree.leaves()])\n",
    "                    label = subtree.label()\n",
    "                    entities.append((entity, label))\n",
    "    except:\n",
    "        # Fallback: simple capitalized word heuristic\n",
    "        entities = []\n",
    "        for sent in tokens:\n",
    "            for word in sent:\n",
    "                if word[0].isupper() and len(word) > 1:\n",
    "                    entities.append((word, 'UNKNOWN'))\n",
    "    \n",
    "    # Create narrative structure\n",
    "    narrative = {\n",
    "        'characters': [],\n",
    "        'character_mentions': defaultdict(int),\n",
    "        'character_network': defaultdict(set),\n",
    "        'dialogue_ratio': 0,\n",
    "        'narrative_pacing': [],\n",
    "        'locations': []\n",
    "    }\n",
    "    \n",
    "    # Extract characters (named entities that are people)\n",
    "    characters = set(name for name, label in entities if label == 'PERSON')\n",
    "    narrative['characters'] = list(characters)\n",
    "    \n",
    "    # Extract locations\n",
    "    locations = set(name for name, label in entities if label in ('GPE', 'LOCATION', 'FACILITY'))\n",
    "    narrative['locations'] = list(locations)\n",
    "    \n",
    "    # Count character mentions\n",
    "    for name, label in entities:\n",
    "        if label == 'PERSON' and name in characters:\n",
    "            narrative['character_mentions'][name] += 1\n",
    "    \n",
    "    # Simple character network construction\n",
    "    # Characters appearing in the same sentence are connected\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sent_characters = set()\n",
    "        for char in characters:\n",
    "            if char in sent:\n",
    "                sent_characters.add(char)\n",
    "        \n",
    "        # Connect all characters in this sentence\n",
    "        for char1 in sent_characters:\n",
    "            for char2 in sent_characters:\n",
    "                if char1 != char2:\n",
    "                    narrative['character_network'][char1].add(char2)\n",
    "    \n",
    "    # Estimate dialogue ratio\n",
    "    dialogue_pattern = re.compile(r'\"[^\"]*\"')\n",
    "    dialogue_matches = dialogue_pattern.findall(text)\n",
    "    dialogue_chars = sum(len(m) for m in dialogue_matches)\n",
    "    narrative['dialogue_ratio'] = dialogue_chars / len(text) if len(text) > 0 else 0\n",
    "    \n",
    "    # Simple narrative pacing measurement\n",
    "    # Divide into segments and measure event density\n",
    "    segment_count = min(10, len(sentences))\n",
    "    segments = np.array_split(sentences, segment_count)\n",
    "    \n",
    "    for segment in segments:\n",
    "        segment_text = ' '.join(segment)\n",
    "        # Count named entities and action verbs as proxy for events\n",
    "        event_count = 0\n",
    "        for sent in segment:\n",
    "            # Count entities in this segment\n",
    "            for name, _ in entities:\n",
    "                if name in sent:\n",
    "                    event_count += 1\n",
    "            # Count action verbs (simplified)\n",
    "            for word in word_tokenize(sent):\n",
    "                if word.lower() in ('run', 'jump', 'fight', 'kill', 'die', 'move', 'attack', 'defend',\n",
    "                                   'go', 'come', 'leave', 'arrive', 'start', 'end', 'begin', 'finish'):\n",
    "                    event_count += 1\n",
    "        \n",
    "        segment_word_count = sum(len(word_tokenize(sent)) for sent in segment)\n",
    "        narrative['narrative_pacing'].append(event_count / segment_word_count if segment_word_count > 0 else 0)\n",
    "    \n",
    "    return narrative\n",
    "\n",
    "# Example narrative text\n",
    "example_narrative = \"\"\"\n",
    "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: \n",
    "once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, \n",
    "\"and what is the use of a book,\" thought Alice \"without pictures or conversations?\"\n",
    "\n",
    "So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), \n",
    "whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, \n",
    "when suddenly a White Rabbit with pink eyes ran close by her.\n",
    "\n",
    "There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say \n",
    "to itself, \"Oh dear! Oh dear! I shall be late!\" (when she thought it over afterwards, it occurred to her that she ought \n",
    "to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of \n",
    "its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind \n",
    "that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with \n",
    "curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole \n",
    "under the hedge.\n",
    "\n",
    "In another moment down went Alice after it, never once considering how in the world she was to get out again.\n",
    "\"\"\"\n",
    "\n",
    "# Extract narrative elements\n",
    "narrative_elements = extract_narrative_elements(example_narrative)\n",
    "\n",
    "# Display results\n",
    "print(\"Character Analysis:\")\n",
    "print(f\"Characters identified: {', '.join(narrative_elements['characters'])}\")\n",
    "print(f\"Locations identified: {', '.join(narrative_elements['locations'])}\")\n",
    "print(f\"Dialogue ratio: {narrative_elements['dialogue_ratio']:.2f}\")\n",
    "\n",
    "# Create character network visualization if characters were found\n",
    "if narrative_elements['character_network']:\n",
    "    G = nx.Graph()\n",
    "    for char in narrative_elements['characters']:\n",
    "        G.add_node(char)\n",
    "    \n",
    "    for char1, connected in narrative_elements['character_network'].items():\n",
    "        for char2 in connected:\n",
    "            G.add_edge(char1, char2)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos, with_labels=True, node_color=\"lightgreen\", \n",
    "            node_size=3000, font_size=10, font_weight=\"bold\")\n",
    "    plt.title(\"Character Relationship Network\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot narrative pacing\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(narrative_elements['narrative_pacing'], marker='o')\n",
    "plt.title(\"Narrative Pacing\")\n",
    "plt.xlabel(\"Segment\")\n",
    "plt.ylabel(\"Event Density\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e101b",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Implementation Approaches for Palimpsest\n",
    "\n",
    "This section outlines practical approaches for implementing the above techniques\n",
    "in the Palimpsest tool, with particular focus on handling large documents efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2543dfd",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Multi-level Analysis Framework\n",
    "\n",
    "To effectively integrate these techniques into Palimpsest, we recommend a layered approach:\n",
    "\n",
    "1. **Preprocessing Pipeline**:\n",
    "   - Document segmentation into logical units (paragraphs, sections)\n",
    "   - Syntactic analysis and fingerprinting at various levels of granularity\n",
    "   - Structural feature extraction\n",
    "   - Narrative element identification\n",
    "\n",
    "2. **Multi-level Analysis Framework**:\n",
    "   - Surface level: Basic syntax features, readability, structural elements\n",
    "   - Middle level: Syntactic patterns, section organization, character networks\n",
    "   - Deep level: Plot arcs, intertextual connections, narrative techniques\n",
    "\n",
    "3. **Efficient Implementation Strategies**:\n",
    "   - Use sparse representations for syntactic features\n",
    "   - Implement incremental analysis for very large documents\n",
    "   - Apply clustering techniques to identify related document sections\n",
    "   - Use multi-threading for independent analysis tasks\n",
    "   \n",
    "4. **Visualization Framework**:\n",
    "   - Syntactic heat maps showing similarity regions\n",
    "   - Document structure comparisons using tree/graph visualizations \n",
    "   - Character network visualizations\n",
    "   - Plot arc comparisons using line charts and overlays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a009f",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Recommendations for Palimpsest\n",
    "\n",
    "Based on our research, we recommend the following approaches for implementing advanced text analysis in Palimpsest:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Key recommendations\n",
    "recommendations = [\n",
    "    (\"Multi-tier Analysis Framework\", \n",
    "     \"Implement a layered approach that allows both quick surface analysis and deep structural comparison\"),\n",
    "    \n",
    "    (\"Adaptive Processing\", \n",
    "     \"Apply more intensive techniques only to promising document sections identified by lightweight methods\"),\n",
    "    \n",
    "    (\"Comparative Visualization\", \n",
    "     \"Develop visualization tools that can show syntactic, structural, and narrative similarities simultaneously\"),\n",
    "    \n",
    "    (\"Incremental Analysis\", \n",
    "     \"Support progressive analysis where results are refined as more intensive techniques are applied\"),\n",
    "    \n",
    "    (\"Cross-domain Integration\", \n",
    "     \"Combine insights from syntactic, structural, and narrative analysis with previously researched string and semantic matching\"),\n",
    "    \n",
    "    (\"Configurable Analysis Pipeline\", \n",
    "     \"Allow users to select which analysis techniques to apply based on their research questions\"),\n",
    "    \n",
    "    (\"Ground-truth Validation\", \n",
    "     \"Include tools for users to validate and correct automated analysis, improving accuracy over time\")\n",
    "]\n",
    "\n",
    "# Display recommendations in a table format\n",
    "recommendation_df = pd.DataFrame(recommendations, columns=[\"Recommendation\", \"Description\"])\n",
    "display(recommendation_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a0a368",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Libraries for Implementation\n",
    "\n",
    "The following libraries provide essential functionality for implementing the proposed techniques in Palimpsest:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b030adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Libraries for implementation\n",
    "libraries = {\n",
    "    \"Syntactic Analysis\": [\n",
    "        (\"spaCy\", \"Industrial-strength NLP with syntactic analysis\"),\n",
    "        (\"NLTK\", \"Natural Language Toolkit with various linguistic tools\"),\n",
    "        (\"CoreNLP\", \"Stanford's NLP suite with advanced parsing\"),\n",
    "        (\"stanza\", \"Stanford NLP's Python interface\"),\n",
    "        (\"textstat\", \"Text statistics and readability metrics\")\n",
    "    ],\n",
    "    \n",
    "    \"Structural Analysis\": [\n",
    "        (\"networkx\", \"Network analysis and visualization\"),\n",
    "        (\"scikit-learn\", \"Machine learning for structure classification\"),\n",
    "        (\"LDA implementations\", \"Topic modeling for section analysis\"),\n",
    "        (\"pygraphviz\", \"Graph visualization\"),\n",
    "        (\"BeautifulSoup\", \"HTML/XML parsing for structured documents\")\n",
    "    ],\n",
    "    \n",
    "    \"Narrative Analysis\": [\n",
    "        (\"BookNLP\", \"NLP pipeline for narrative text\"),\n",
    "        (\"LitBank\", \"Literary entity and event extraction\"),\n",
    "        (\"NLTK's sentiment tools\", \"Sentiment analysis for plot arcs\"),\n",
    "        (\"NetworkX\", \"Character relationship network analysis\"),\n",
    "        (\"SpanMarker\", \"Entity and span detection models\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, libs in libraries.items():\n",
    "    md(f\"### {category}\")\n",
    "    \n",
    "    table_data = []\n",
    "    for name, desc in libs:\n",
    "        table_data.append([name, desc])\n",
    "    \n",
    "    df = pd.DataFrame(table_data, columns=[\"Library\", \"Description\"])\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d765ae",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Conclusion\n",
    "\n",
    "This research has identified advanced techniques for syntactic, structural, and plot analysis \n",
    "that can be integrated into Palimpsest for comprehensive text comparison. By combining these \n",
    "approaches with the previously researched string matching and semantic matching techniques, \n",
    "Palimpsest will be able to provide multi-layered analysis of textual similarities and differences.\n",
    "\n",
    "The integration of these techniques will allow users to:\n",
    "1. Identify authorial fingerprints through syntactic analysis\n",
    "2. Discover structural patterns and organizational influences\n",
    "3. Compare narrative techniques and plot development across texts\n",
    "4. Visualize multi-faceted textual relationships\n",
    "\n",
    "This comprehensive approach positions Palimpsest as a powerful tool for literary analysis, \n",
    "comparative literature studies, and historical document research.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
